{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "# from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and Explore Data\n",
    "# Load datasets\n",
    "X_train = pd.read_csv(\"../04_modelling/dataset/X_train.csv\")\n",
    "y_train = pd.read_csv(\"../04_modelling/dataset/y_train.csv\")\n",
    "# X_val = pd.read_csv(\"../04_modelling/dataset/X_val.csv\")\n",
    "# y_val = pd.read_csv(\"../04_modelling/dataset/y_val.csv\")\n",
    "X_test = pd.read_csv(\"../04_modelling/dataset/X_test.csv\")\n",
    "y_test = pd.read_csv(\"../04_modelling/dataset/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5459, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = y_train['yearly_compensation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = target_names.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_classifier(clf, param_grid, title):\n",
    "#     # -----------------------------------------------------\n",
    "#     cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n",
    "#     # Randomized grid search\n",
    "#     n_iter_search = 10\n",
    "#     gs = RandomizedSearchCV(\n",
    "#         clf,\n",
    "#         param_distributions=param_grid,\n",
    "#         n_iter=n_iter_search,\n",
    "#         cv=cv,\n",
    "#         scoring='accuracy'\n",
    "#     )\n",
    "#     # -----------------------------------------------------\n",
    "#     # Train model\n",
    "#     gs.fit(X_train, y_train)  \n",
    "#     print(\"The best parameters are %s\" % (gs.best_params_)) \n",
    "#     # Predict on test set\n",
    "#     y_pred = gs.best_estimator_.predict(X_test)\n",
    "#     # Get Probability estimates\n",
    "#     y_prob = gs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "#     # -----------------------------------------------------\n",
    "#     print('Accuracy score: %.2f%%' % (accuracy_score(y_test, y_pred)*100))  \n",
    "#     print('Precision score: %.2f%%' % (precision_score(y_test, y_pred, average='weighted')*100))\n",
    "#     print('Recall score: %.2f%%' % (recall_score(y_test, y_pred, average='weighted')*100))\n",
    "#     # -----------------------------------------------------\n",
    "    # Plot confusion matrix\n",
    "    # fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    # cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # # Ensure target names match unique classes\n",
    "    # target_names = sorted(set(y_test))  # Unique classes\n",
    "    \n",
    "    # sns.heatmap(cm, annot=True, cbar=False, fmt=\"d\", linewidths=.5, cmap=\"Blues\", ax=ax1)\n",
    "    # ax1.set_title(\"Confusion Matrix\")\n",
    "    # ax1.set_xlabel(\"Predicted class\")\n",
    "    # ax1.set_ylabel(\"Actual class\")\n",
    "    # ax1.set_xticklabels(target_names, rotation=45)\n",
    "    # ax1.set_yticklabels(target_names)\n",
    "    # fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(clf, param_grid, title):\n",
    "    # -----------------------------------------------------\n",
    "    # Cross-Validation Setup\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)  # 5-fold CV for better generalization\n",
    "    n_iter_search = 10  # Number of parameter combinations for RandomizedSearch\n",
    "    \n",
    "    gs = RandomizedSearchCV(\n",
    "        clf,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=n_iter_search,\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        return_train_score=True\n",
    "    )\n",
    "    # -----------------------------------------------------\n",
    "    # Perform Cross-Validation and Hyperparameter Tuning\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(f\"\\n--- Cross-Validation Results ({title}) ---\")\n",
    "    print(\"The best parameters are:\", gs.best_params_)\n",
    "    print(\"Mean cross-validation accuracy: %.2f%%\" % (gs.best_score_ * 100))\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # Evaluate Model on Test and Validation Sets\n",
    "    print(\"\\n--- Test and Validation Results ---\")\n",
    "    \n",
    "    # Predict on Test Set\n",
    "    y_test_pred = gs.best_estimator_.predict(X_test)\n",
    "    y_test_prob = gs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Predict on Validation Set\n",
    "    # y_val_pred = gs.best_estimator_.predict(X_val)\n",
    "    # y_val_prob = gs.best_estimator_.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Test Set Metrics\n",
    "    print(\"\\n--- Test Metrics ---\")\n",
    "    print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_test_pred) * 100))\n",
    "    print('Precision: %.2f%%' % (precision_score(y_test, y_test_pred, average='weighted') * 100))\n",
    "    print('Recall: %.2f%%' % (recall_score(y_test, y_test_pred, average='weighted') * 100))\n",
    "    \n",
    "    # Validation Set Metrics\n",
    "    # print(\"\\n--- Validation Metrics ---\")\n",
    "    # print('Accuracy: %.2f%%' % (accuracy_score(y_val, y_val_pred) * 100))\n",
    "    # print('Precision: %.2f%%' % (precision_score(y_val, y_val_pred, average='weighted') * 100))\n",
    "    # print('Recall: %.2f%%' % (recall_score(y_val, y_val_pred, average='weighted') * 100))\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # Confusion Matrices (optional for analysis)\n",
    "    # print(\"\\nConfusion Matrix (Test):\")\n",
    "    # print(confusion_matrix(y_test, y_test_pred))\n",
    "    # print(\"\\nConfusion Matrix (Validation):\")\n",
    "    # print(confusion_matrix(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def run_regressor(regressor, param_grid, title):\n",
    "    # -----------------------------------------------------\n",
    "    # Cross-Validation Setup\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)  # K-Fold CV for regression\n",
    "    n_iter_search = 10  # Number of parameter combinations for RandomizedSearch\n",
    "\n",
    "    gs = RandomizedSearchCV(\n",
    "        regressor,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=n_iter_search,\n",
    "        cv=cv,\n",
    "        scoring='neg_root_mean_squared_error',  # For regression, use RMSE\n",
    "        return_train_score=True\n",
    "    )\n",
    "    # -----------------------------------------------------\n",
    "    # Perform Cross-Validation and Hyperparameter Tuning\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(f\"\\n--- Cross-Validation Results ({title}) ---\")\n",
    "    print(\"The best parameters are:\", gs.best_params_)\n",
    "    print(\"Mean cross-validation RMSE: %.4f\" % (-gs.best_score_))\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # Evaluate Model on Test Set\n",
    "    print(\"\\n--- Test Results ---\")\n",
    "    \n",
    "    # Predict on Test Set\n",
    "    y_test_pred = gs.best_estimator_.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    r2 = r2_score(y_test, y_test_pred)\n",
    "    # mape = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100  # Mean Absolute Percentage Error\n",
    "\n",
    "    print('Test RMSE: %.4f' % rmse)\n",
    "    print('Test MAE: %.4f' % mae)\n",
    "    print('Test R² (Accuracy): %.2f%%' % (r2 * 100))\n",
    "    # print('Test MAPE: %.2f%%' % mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cross-Validation Results (LightGBM Regressor) ---\n",
      "The best parameters are: {'num_leaves': 31, 'n_estimators': 1000, 'learning_rate': 0.1, 'feature_fraction': 0.7, 'bagging_fraction': 0.8}\n",
      "Mean cross-validation RMSE: 2.0086\n",
      "\n",
      "--- Test Results ---\n",
      "Test RMSE: 1.9360\n",
      "Test MAE: 1.4020\n",
      "Test R² (Accuracy): 93.47%\n"
     ]
    }
   ],
   "source": [
    "# Define LightGBM parameters for tuning\n",
    "param_grid_lgbm = {\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'bagging_fraction': [0.5, 0.6, 0.8],\n",
    "    'feature_fraction': [0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "# Initialize LightGBM regressor\n",
    "lgbm_regressor = lgb.LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    metric=\"rmse\",\n",
    "    bagging_seed=42,\n",
    "    verbosity=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Call the helper function\n",
    "run_regressor(lgbm_regressor, param_grid_lgbm, \"LightGBM Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cross-Validation Results (Logistic Regression) ---\n",
      "The best parameters are: {'solver': 'lbfgs', 'penalty': 'l2'}\n",
      "Mean cross-validation accuracy: 70.88%\n",
      "\n",
      "--- Test and Validation Results ---\n",
      "\n",
      "--- Test Metrics ---\n",
      "Accuracy: 73.97%\n",
      "Precision: 72.85%\n",
      "Recall: 73.97%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "param_grid_lr = {'penalty': ['l2'],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "run_classifier(lr, param_grid_lr, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cross-Validation Results (Decision Tree) ---\n",
      "The best parameters are: {'splitter': 'random', 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None, 'max_depth': 11, 'criterion': 'gini'}\n",
      "Mean cross-validation accuracy: 42.09%\n",
      "\n",
      "--- Test and Validation Results ---\n",
      "\n",
      "--- Test Metrics ---\n",
      "Accuracy: 41.86%\n",
      "Precision: 40.10%\n",
      "Recall: 41.86%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "param_grid_dtree = {'criterion': ['gini', 'entropy'],\n",
    "              'splitter': ['best', 'random'],\n",
    "              'max_depth': np.arange(1, 20, 2),\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 2, 4, 10],\n",
    "              'max_features': ['auto', 'sqrt', 'log2', None]}\n",
    "\n",
    "run_classifier(dtree, param_grid_dtree, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cross-Validation Results (Decision Tree) ---\n",
      "The best parameters are: {'splitter': 'random', 'min_samples_split': 20, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 21, 'criterion': 'gini', 'class_weight': 'balanced'}\n",
      "Mean cross-validation accuracy: 37.44%\n",
      "\n",
      "--- Test and Validation Results ---\n",
      "\n",
      "--- Test Metrics ---\n",
      "Accuracy: 36.35%\n",
      "Precision: 42.44%\n",
      "Recall: 36.35%\n"
     ]
    }
   ],
   "source": [
    "dtree_2 = DecisionTreeClassifier()\n",
    "\n",
    "param_grid_dtree_2 = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # Include 'log_loss' for classification\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': np.arange(1, 50, 5),  # Increase depth range\n",
    "    'min_samples_split': [2, 5, 10, 20],  # Larger values for more generalization\n",
    "    'min_samples_leaf': [1, 2, 4, 10, 20],  # Larger leaves for pruning\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None],  # Adjust based on dataset size\n",
    "    'class_weight': [None, 'balanced']  # Try balancing class weights\n",
    "}\n",
    "\n",
    "run_classifier(dtree, param_grid_dtree_2, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cross-Validation Results (Decision Tree) ---\n",
      "The best parameters are: {'splitter': 'best', 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 27, 'criterion': 'entropy', 'class_weight': 'balanced'}\n",
      "Mean cross-validation accuracy: 42.25%\n",
      "\n",
      "--- Test and Validation Results ---\n",
      "\n",
      "--- Test Metrics ---\n",
      "Accuracy: 44.87%\n",
      "Precision: 46.58%\n",
      "Recall: 44.87%\n"
     ]
    }
   ],
   "source": [
    "dtree_3 = DecisionTreeClassifier()\n",
    "\n",
    "param_grid_dtree_3 = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': np.arange(1, 30, 2),  # Extend range\n",
    "    'min_samples_split': [2, 5, 10, 20, 50],  # Include larger splits\n",
    "    'min_samples_leaf': [1, 2, 4, 10, 20],  # Include larger leaf sizes\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'class_weight': [None, 'balanced'],  # Try balancing classes\n",
    "}\n",
    "\n",
    "run_classifier(dtree_3, param_grid_dtree_3, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import tree\n",
    "# target_names = sorted(set(y_test))\n",
    "# fig = plt.figure(figsize=(25,20))\n",
    "# _ = tree.plot_tree(dtree_3,\n",
    "#                    feature_names=X_train.columns,\n",
    "#                    class_names=target_names,\n",
    "#                    filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_4 = DecisionTreeClassifier()\n",
    "\n",
    "param_grid_dtree_4 = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # Different impurity measures\n",
    "    'splitter': ['best', 'random'],  # Best or random split selection\n",
    "    'max_depth': [2, 3, 5, 10, 20],  # Fix range definition\n",
    "    'min_samples_split': [2, 5, 10, 20, 50, 100],  # Extended to larger values for robustness\n",
    "    'min_samples_leaf': [1, 5, 10, 20, 50, 100],  # Include smaller leaf sizes for granular splits\n",
    "    'max_features': [None, 'sqrt', 'log2'],  # Different feature selection strategies\n",
    "    'class_weight': [None, 'balanced'],  # Account for imbalanced classes\n",
    "}\n",
    "\n",
    "run_classifier(dtree_4, param_grid_dtree_4, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "param_grid_rf = {'n_estimators': [100, 200],\n",
    "              'max_depth': [10, 20, 100, None],\n",
    "              'max_features': ['auto', 'sqrt', None],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 2, 4, 10],\n",
    "              'bootstrap': [True, False],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "run_classifier(rf, param_grid_rf, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "param_grid = {'hidden_layer_sizes': [(10,), (50,), (10, 10), (50, 50)],\n",
    "             'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "             'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "             'alpha': np.logspace(-5, 3, 5),\n",
    "             'learning_rate': ['constant', 'invscaling','adaptive'],\n",
    "             'max_iter': [100, 500, 1000]}\n",
    "\n",
    "run_classifier(mlp, param_grid, 'Neural Net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
