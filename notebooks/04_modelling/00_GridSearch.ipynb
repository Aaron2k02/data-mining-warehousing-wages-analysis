{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "import optuna\n",
    "from model_utils import run_classifier\n",
    "import joblib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "X_train = pd.read_csv(\"../04_modelling/dataset/X_train.csv\")\n",
    "y_train = pd.read_csv(\"../04_modelling/dataset/y_train.csv\").squeeze()  # Convert to Series if needed\n",
    "X_test = pd.read_csv(\"../04_modelling/dataset/X_test.csv\")\n",
    "y_test = pd.read_csv(\"../04_modelling/dataset/y_test.csv\").squeeze()    # Convert to Series if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame to store model results\n",
    "model_records = pd.DataFrame(columns=[\"Model Name\", \"Hyperparameters\", \"Test Accuracy\", \"Test Precision\", \"Test Recall\"])\n",
    "\n",
    "def record_trained_model(model_name, params, mean_cv_accuracy, test_metrics):\n",
    "    \"\"\"\n",
    "    Record a trained model's details in a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model.\n",
    "        params (dict): Hyperparameters used for training.\n",
    "        mean_cv_accuracy (float): Mean cross-validation accuracy (optional).\n",
    "        test_metrics (dict): Test metrics such as accuracy, precision, and recall.\n",
    "    \"\"\"\n",
    "    global model_records  # Use the global DataFrame\n",
    "\n",
    "    # Create a new record\n",
    "    record = {\n",
    "        \"Model Name\": model_name,\n",
    "        \"Hyperparameters\": params,\n",
    "        \"Test Accuracy\": f\"{test_metrics['accuracy']:.2%}\",\n",
    "        \"Test Precision\": f\"{test_metrics['precision']:.2%}\",\n",
    "        \"Test Recall\": f\"{test_metrics['recall']:.2%}\",\n",
    "    }\n",
    "\n",
    "    # Append the record to the DataFrame\n",
    "    model_records = pd.concat([model_records, pd.DataFrame([record])], ignore_index=True)\n",
    "\n",
    "    print(f\"Model '{model_name}' recorded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default MLPClassifier\n",
    "dtree_default = DecisionTreeClassifier()\n",
    "\n",
    "print(\"Decision Tree with Default Parameters\")\n",
    "best_model_dtree_default = run_classifier(dtree_default, {}, X_train, y_train, X_test, y_test, \"Default Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record default parameter model performance\n",
    "default_test_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, best_model_dtree_default.predict(X_test)),\n",
    "    \"precision\": precision_score(y_test, best_model_dtree_default.predict(X_test), average='weighted'),\n",
    "    \"recall\": recall_score(y_test, best_model_dtree_default.predict(X_test), average='weighted')\n",
    "}\n",
    "record_trained_model(\"Decision Tree with Default Parameters\", {}, np.nan, default_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree with Hyperparameter Tuning\n",
    "dtree_tuned = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': np.arange(1, 20, 2),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 10]\n",
    "}\n",
    "\n",
    "best_model_dtree_tuned = run_classifier(dtree_tuned, param_grid, X_train, y_train, X_test, y_test, \"Tuned Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record tuned parameter model performance\n",
    "tuned_test_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, best_model_dtree_tuned.predict(X_test)),\n",
    "    \"precision\": precision_score(y_test, best_model_dtree_tuned.predict(X_test), average='weighted'),\n",
    "    \"recall\": recall_score(y_test, best_model_dtree_tuned.predict(X_test), average='weighted')\n",
    "}\n",
    "record_trained_model(\"Decision Tree with Tuned Parameters\", best_model_dtree_tuned.get_params(), np.nan, tuned_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Decision Tree Parameters using Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    criterion = trial.suggest_categorical(\"criterion\", ['gini', 'entropy'])\n",
    "    splitter = trial.suggest_categorical(\"splitter\", ['best', 'random'])\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "\n",
    "    # Create the model with the sampled parameters\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        splitter=splitter,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=123\n",
    "    )\n",
    "\n",
    "    # Evaluate the model using cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "\n",
    "    return mean_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best parameters\n",
    "best_params = study.best_params\n",
    "print(\"\\nBest Hyperparameters from Optuna:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree with Optuna Tuned Parameters\n",
    "best_model_dtree_optuna = DecisionTreeClassifier(**best_params, random_state=123)\n",
    "\n",
    "best_model_dtree_optuna = run_classifier(dtree_tuned, param_grid, X_train, y_train, X_test, y_test, \"Tuned Decision Tree\")\n",
    "best_model_dtree_optuna.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Optuna-tuned model\n",
    "optuna_test_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, best_model_dtree_optuna.predict(X_test)),\n",
    "    \"precision\": precision_score(y_test, best_model_dtree_optuna.predict(X_test), average='weighted'),\n",
    "    \"recall\": recall_score(y_test, best_model_dtree_optuna.predict(X_test), average='weighted')\n",
    "}\n",
    "record_trained_model(\"Decision Tree with Optuna Parameters\", best_params, np.nan, optuna_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: pruned decision tree\n",
    "dtree_pruned = DecisionTreeClassifier(**best_params, random_state=123, ccp_alpha=0.01)\n",
    "print(\"Training Decision Tree with Advanced Techniques (e.g., Pruning)...\")\n",
    "dtree_pruned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the advanced pruned model\n",
    "test_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, dtree_pruned.predict(X_test)),\n",
    "    \"precision\": precision_score(y_test, dtree_pruned.predict(X_test), average='weighted'),\n",
    "    \"recall\": recall_score(y_test, dtree_pruned.predict(X_test), average='weighted')\n",
    "}\n",
    "record_trained_model(\"Pruned Decision Tree with Optuna Parameters\", dtree_pruned.get_params(), np.nan, test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Results in a Table\n",
    "results_df = pd.DataFrame(model_records)\n",
    "results_df.sort_values(by=\"Test Accuracy\", ascending=True, inplace=True)\n",
    "print(\"Model Performance Comparison:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on RMSE\n",
    "best_model_info = results_df.iloc[0]\n",
    "print(\"\\nBest Model:\")\n",
    "best_model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluate and Compare Models\")\n",
    "models = {\n",
    "    \"Default Decision Tree\": dtree_default,\n",
    "    \"Tuned Decision Tree\": dtree_tuned,\n",
    "    \"Optuna Tuned Desicion Tree\": best_model_dtree_optuna,\n",
    "    \"Pruned Optuna Tuned Desicion Tree\": dtree_pruned,\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Evaluating {name} ---\")\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Precision: {precision:.2%}\")\n",
    "    print(f\"Recall: {recall:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the highest accuracy\n",
    "best_model_name = max(models, key=lambda name: accuracy_score(y_test, models[name].predict(X_test)))\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nThe Best Model is '{best_model_name}'\")\n",
    "print(best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path, model_name=\"dtree_model.pkl\"):\n",
    "    \"\"\"\n",
    "    Save the trained model to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model object.\n",
    "        path (str): Directory path to save the model.\n",
    "        model_name (str): File name for the saved model.\n",
    "    \"\"\"\n",
    "    # Ensure the path exists\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Save the model\n",
    "    file_path = os.path.join(path, model_name)\n",
    "    joblib.dump(model, file_path)\n",
    "    print(f\"Model saved successfully at {file_path}!\")\n",
    "\n",
    "# Save the selected best model\n",
    "save_model(best_model, path=\"../04_modelling/models/\", model_name=\"dtree_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Import Necessary Libraries\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "# from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# import seaborn as sns\n",
    "# import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# # from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Load and Explore Data\n",
    "# Load datasets\n",
    "# X_train = pd.read_csv(\"../04_modelling/dataset/X_train.csv\")\n",
    "# y_train = pd.read_csv(\"../04_modelling/dataset/y_train.csv\")\n",
    "# # X_val = pd.read_csv(\"../04_modelling/dataset/X_val.csv\")\n",
    "# # y_val = pd.read_csv(\"../04_modelling/dataset/y_val.csv\")\n",
    "# X_test = pd.read_csv(\"../04_modelling/dataset/X_test.csv\")\n",
    "# y_test = pd.read_csv(\"../04_modelling/dataset/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5459, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_names = y_train['yearly_compensation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_names = target_names.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_classifier(clf, param_grid, title):\n",
    "#     # -----------------------------------------------------\n",
    "#     cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n",
    "#     # Randomized grid search\n",
    "#     n_iter_search = 10\n",
    "#     gs = RandomizedSearchCV(\n",
    "#         clf,\n",
    "#         param_distributions=param_grid,\n",
    "#         n_iter=n_iter_search,\n",
    "#         cv=cv,\n",
    "#         scoring='accuracy'\n",
    "#     )\n",
    "#     # -----------------------------------------------------\n",
    "#     # Train model\n",
    "#     gs.fit(X_train, y_train)  \n",
    "#     print(\"The best parameters are %s\" % (gs.best_params_)) \n",
    "#     # Predict on test set\n",
    "#     y_pred = gs.best_estimator_.predict(X_test)\n",
    "#     # Get Probability estimates\n",
    "#     y_prob = gs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "#     # -----------------------------------------------------\n",
    "#     print('Accuracy score: %.2f%%' % (accuracy_score(y_test, y_pred)*100))  \n",
    "#     print('Precision score: %.2f%%' % (precision_score(y_test, y_pred, average='weighted')*100))\n",
    "#     print('Recall score: %.2f%%' % (recall_score(y_test, y_pred, average='weighted')*100))\n",
    "#     # -----------------------------------------------------\n",
    "    # Plot confusion matrix\n",
    "    # fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    # cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # # Ensure target names match unique classes\n",
    "    # target_names = sorted(set(y_test))  # Unique classes\n",
    "    \n",
    "    # sns.heatmap(cm, annot=True, cbar=False, fmt=\"d\", linewidths=.5, cmap=\"Blues\", ax=ax1)\n",
    "    # ax1.set_title(\"Confusion Matrix\")\n",
    "    # ax1.set_xlabel(\"Predicted class\")\n",
    "    # ax1.set_ylabel(\"Actual class\")\n",
    "    # ax1.set_xticklabels(target_names, rotation=45)\n",
    "    # ax1.set_yticklabels(target_names)\n",
    "    # fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_classifier(clf, param_grid, title):\n",
    "#     # -----------------------------------------------------\n",
    "#     # Cross-Validation Setup\n",
    "#     cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)  # 5-fold CV for better generalization\n",
    "#     n_iter_search = 10  # Number of parameter combinations for RandomizedSearch\n",
    "    \n",
    "#     gs = RandomizedSearchCV(\n",
    "#         clf,\n",
    "#         param_distributions=param_grid,\n",
    "#         n_iter=n_iter_search,\n",
    "#         cv=cv,\n",
    "#         scoring='accuracy',\n",
    "#         return_train_score=True\n",
    "#     )\n",
    "#     # -----------------------------------------------------\n",
    "#     # Perform Cross-Validation and Hyperparameter Tuning\n",
    "#     gs.fit(X_train, y_train)\n",
    "#     print(f\"\\n--- Cross-Validation Results ({title}) ---\")\n",
    "#     print(\"The best parameters are:\", gs.best_params_)\n",
    "#     print(\"Mean cross-validation accuracy: %.2f%%\" % (gs.best_score_ * 100))\n",
    "    \n",
    "#     # -----------------------------------------------------\n",
    "#     # Evaluate Model on Test and Validation Sets\n",
    "#     print(\"\\n--- Test and Validation Results ---\")\n",
    "    \n",
    "#     # Predict on Test Set\n",
    "#     y_test_pred = gs.best_estimator_.predict(X_test)\n",
    "#     y_test_prob = gs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "#     # Predict on Validation Set\n",
    "#     # y_val_pred = gs.best_estimator_.predict(X_val)\n",
    "#     # y_val_prob = gs.best_estimator_.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "#     # Test Set Metrics\n",
    "#     print(\"\\n--- Test Metrics ---\")\n",
    "#     print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_test_pred) * 100))\n",
    "#     print('Precision: %.2f%%' % (precision_score(y_test, y_test_pred, average='weighted') * 100))\n",
    "#     print('Recall: %.2f%%' % (recall_score(y_test, y_test_pred, average='weighted') * 100))\n",
    "    \n",
    "#     # Validation Set Metrics\n",
    "#     # print(\"\\n--- Validation Metrics ---\")\n",
    "#     # print('Accuracy: %.2f%%' % (accuracy_score(y_val, y_val_pred) * 100))\n",
    "#     # print('Precision: %.2f%%' % (precision_score(y_val, y_val_pred, average='weighted') * 100))\n",
    "#     # print('Recall: %.2f%%' % (recall_score(y_val, y_val_pred, average='weighted') * 100))\n",
    "    \n",
    "#     # -----------------------------------------------------\n",
    "#     # Confusion Matrices (optional for analysis)\n",
    "#     # print(\"\\nConfusion Matrix (Test):\")\n",
    "#     # print(confusion_matrix(y_test, y_test_pred))\n",
    "#     # print(\"\\nConfusion Matrix (Validation):\")\n",
    "#     # print(confusion_matrix(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# import numpy as np\n",
    "\n",
    "# def run_regressor(regressor, param_grid, title):\n",
    "#     # -----------------------------------------------------\n",
    "#     # Cross-Validation Setup\n",
    "#     cv = KFold(n_splits=5, shuffle=True, random_state=123)  # K-Fold CV for regression\n",
    "#     n_iter_search = 10  # Number of parameter combinations for RandomizedSearch\n",
    "\n",
    "#     gs = RandomizedSearchCV(\n",
    "#         regressor,\n",
    "#         param_distributions=param_grid,\n",
    "#         n_iter=n_iter_search,\n",
    "#         cv=cv,\n",
    "#         scoring='neg_root_mean_squared_error',  # For regression, use RMSE\n",
    "#         return_train_score=True\n",
    "#     )\n",
    "#     # -----------------------------------------------------\n",
    "#     # Perform Cross-Validation and Hyperparameter Tuning\n",
    "#     gs.fit(X_train, y_train)\n",
    "#     print(f\"\\n--- Cross-Validation Results ({title}) ---\")\n",
    "#     print(\"The best parameters are:\", gs.best_params_)\n",
    "#     print(\"Mean cross-validation RMSE: %.4f\" % (-gs.best_score_))\n",
    "    \n",
    "#     # -----------------------------------------------------\n",
    "#     # Evaluate Model on Test Set\n",
    "#     print(\"\\n--- Test Results ---\")\n",
    "    \n",
    "#     # Predict on Test Set\n",
    "#     y_test_pred = gs.best_estimator_.predict(X_test)\n",
    "#     rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "#     mae = mean_absolute_error(y_test, y_test_pred)\n",
    "#     r2 = r2_score(y_test, y_test_pred)\n",
    "#     # mape = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100  # Mean Absolute Percentage Error\n",
    "\n",
    "#     print('Test RMSE: %.4f' % rmse)\n",
    "#     print('Test MAE: %.4f' % mae)\n",
    "#     print('Test R² (Accuracy): %.2f%%' % (r2 * 100))\n",
    "#     # print('Test MAPE: %.2f%%' % mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cross-Validation Results (LightGBM Regressor) ---\n",
      "The best parameters are: {'num_leaves': 31, 'n_estimators': 1000, 'learning_rate': 0.1, 'feature_fraction': 0.7, 'bagging_fraction': 0.8}\n",
      "Mean cross-validation RMSE: 2.0086\n",
      "\n",
      "--- Test Results ---\n",
      "Test RMSE: 1.9360\n",
      "Test MAE: 1.4020\n",
      "Test R² (Accuracy): 93.47%\n"
     ]
    }
   ],
   "source": [
    "# # Define LightGBM parameters for tuning\n",
    "# param_grid_lgbm = {\n",
    "#     'num_leaves': [31, 50, 70],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'n_estimators': [100, 500, 1000],\n",
    "#     'bagging_fraction': [0.5, 0.6, 0.8],\n",
    "#     'feature_fraction': [0.5, 0.7, 0.9]\n",
    "# }\n",
    "\n",
    "# # Initialize LightGBM regressor\n",
    "# lgbm_regressor = lgb.LGBMRegressor(\n",
    "#     objective=\"regression\",\n",
    "#     metric=\"rmse\",\n",
    "#     bagging_seed=42,\n",
    "#     verbosity=-1,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Call the helper function\n",
    "# run_regressor(lgbm_regressor, param_grid_lgbm, \"LightGBM Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cross-Validation Results (Logistic Regression) ---\n",
      "The best parameters are: {'solver': 'lbfgs', 'penalty': 'l2'}\n",
      "Mean cross-validation accuracy: 70.88%\n",
      "\n",
      "--- Test and Validation Results ---\n",
      "\n",
      "--- Test Metrics ---\n",
      "Accuracy: 73.97%\n",
      "Precision: 72.85%\n",
      "Recall: 73.97%\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# lr = LogisticRegression()\n",
    "\n",
    "# param_grid_lr = {'penalty': ['l2'],\n",
    "#               'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "# run_classifier(lr, param_grid_lr, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cross-Validation Results (Decision Tree) ---\n",
      "The best parameters are: {'splitter': 'random', 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None, 'max_depth': 11, 'criterion': 'gini'}\n",
      "Mean cross-validation accuracy: 42.09%\n",
      "\n",
      "--- Test and Validation Results ---\n",
      "\n",
      "--- Test Metrics ---\n",
      "Accuracy: 41.86%\n",
      "Precision: 40.10%\n",
      "Recall: 41.86%\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# import numpy as np\n",
    "\n",
    "# dtree = DecisionTreeClassifier()\n",
    "\n",
    "# param_grid_dtree = {'criterion': ['gini', 'entropy'],\n",
    "#               'splitter': ['best', 'random'],\n",
    "#               'max_depth': np.arange(1, 20, 2),\n",
    "#               'min_samples_split': [2, 5, 10],\n",
    "#               'min_samples_leaf': [1, 2, 4, 10],\n",
    "#               'max_features': ['auto', 'sqrt', 'log2', None]}\n",
    "\n",
    "# run_classifier(dtree, param_grid_dtree, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cross-Validation Results (Decision Tree) ---\n",
      "The best parameters are: {'splitter': 'random', 'min_samples_split': 20, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 21, 'criterion': 'gini', 'class_weight': 'balanced'}\n",
      "Mean cross-validation accuracy: 37.44%\n",
      "\n",
      "--- Test and Validation Results ---\n",
      "\n",
      "--- Test Metrics ---\n",
      "Accuracy: 36.35%\n",
      "Precision: 42.44%\n",
      "Recall: 36.35%\n"
     ]
    }
   ],
   "source": [
    "# dtree_2 = DecisionTreeClassifier()\n",
    "\n",
    "# param_grid_dtree_2 = {\n",
    "#     'criterion': ['gini', 'entropy', 'log_loss'],  # Include 'log_loss' for classification\n",
    "#     'splitter': ['best', 'random'],\n",
    "#     'max_depth': np.arange(1, 50, 5),  # Increase depth range\n",
    "#     'min_samples_split': [2, 5, 10, 20],  # Larger values for more generalization\n",
    "#     'min_samples_leaf': [1, 2, 4, 10, 20],  # Larger leaves for pruning\n",
    "#     'max_features': ['auto', 'sqrt', 'log2', None],  # Adjust based on dataset size\n",
    "#     'class_weight': [None, 'balanced']  # Try balancing class weights\n",
    "# }\n",
    "\n",
    "# run_classifier(dtree, param_grid_dtree_2, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cross-Validation Results (Decision Tree) ---\n",
      "The best parameters are: {'splitter': 'best', 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 27, 'criterion': 'entropy', 'class_weight': 'balanced'}\n",
      "Mean cross-validation accuracy: 42.25%\n",
      "\n",
      "--- Test and Validation Results ---\n",
      "\n",
      "--- Test Metrics ---\n",
      "Accuracy: 44.87%\n",
      "Precision: 46.58%\n",
      "Recall: 44.87%\n"
     ]
    }
   ],
   "source": [
    "# dtree_3 = DecisionTreeClassifier()\n",
    "\n",
    "# param_grid_dtree_3 = {\n",
    "#     'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "#     'splitter': ['best', 'random'],\n",
    "#     'max_depth': np.arange(1, 30, 2),  # Extend range\n",
    "#     'min_samples_split': [2, 5, 10, 20, 50],  # Include larger splits\n",
    "#     'min_samples_leaf': [1, 2, 4, 10, 20],  # Include larger leaf sizes\n",
    "#     'max_features': [None, 'sqrt', 'log2'],\n",
    "#     'class_weight': [None, 'balanced'],  # Try balancing classes\n",
    "# }\n",
    "\n",
    "# run_classifier(dtree_3, param_grid_dtree_3, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import tree\n",
    "# target_names = sorted(set(y_test))\n",
    "# fig = plt.figure(figsize=(25,20))\n",
    "# _ = tree.plot_tree(dtree_3,\n",
    "#                    feature_names=X_train.columns,\n",
    "#                    class_names=target_names,\n",
    "#                    filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtree_4 = DecisionTreeClassifier()\n",
    "\n",
    "# param_grid_dtree_4 = {\n",
    "#     'criterion': ['gini', 'entropy', 'log_loss'],  # Different impurity measures\n",
    "#     'splitter': ['best', 'random'],  # Best or random split selection\n",
    "#     'max_depth': [2, 3, 5, 10, 20],  # Fix range definition\n",
    "#     'min_samples_split': [2, 5, 10, 20, 50, 100],  # Extended to larger values for robustness\n",
    "#     'min_samples_leaf': [1, 5, 10, 20, 50, 100],  # Include smaller leaf sizes for granular splits\n",
    "#     'max_features': [None, 'sqrt', 'log2'],  # Different feature selection strategies\n",
    "#     'class_weight': [None, 'balanced'],  # Account for imbalanced classes\n",
    "# }\n",
    "\n",
    "# run_classifier(dtree_4, param_grid_dtree_4, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# param_grid_rf = {'n_estimators': [100, 200],\n",
    "#               'max_depth': [10, 20, 100, None],\n",
    "#               'max_features': ['auto', 'sqrt', None],\n",
    "#               'min_samples_split': [2, 5, 10],\n",
    "#               'min_samples_leaf': [1, 2, 4, 10],\n",
    "#               'bootstrap': [True, False],\n",
    "#               'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# run_classifier(rf, param_grid_rf, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# mlp = MLPClassifier()\n",
    "\n",
    "# param_grid = {'hidden_layer_sizes': [(10,), (50,), (10, 10), (50, 50)],\n",
    "#              'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "#              'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "#              'alpha': np.logspace(-5, 3, 5),\n",
    "#              'learning_rate': ['constant', 'invscaling','adaptive'],\n",
    "#              'max_iter': [100, 500, 1000]}\n",
    "\n",
    "# run_classifier(mlp, param_grid, 'Neural Net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Necessary Libraries\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt  # For plotting\n",
    "# import h2o\n",
    "# from h2o.grid.grid_search import H2OGridSearch\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import optuna  # For hyperparameter optimization\n",
    "\n",
    "# # Initialize H2O cluster\n",
    "# h2o.init()\n",
    "\n",
    "# # Step 1: Load and Prepare Data\n",
    "# X_train = pd.read_csv(\"../04_modelling/dataset/X_train.csv\")\n",
    "# y_train = pd.read_csv(\"../04_modelling/dataset/y_train.csv\")\n",
    "# X_val = pd.read_csv(\"../04_modelling/dataset/X_val.csv\")\n",
    "# y_val = pd.read_csv(\"../04_modelling/dataset/y_val.csv\")\n",
    "# X_test = pd.read_csv(\"../04_modelling/dataset/X_test.csv\")\n",
    "# y_test = pd.read_csv(\"../04_modelling/dataset/y_test.csv\")\n",
    "\n",
    "# train_df = pd.concat([X_train, y_train], axis=1)\n",
    "# val_df = pd.concat([X_val, y_val], axis=1)\n",
    "# test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# train_h2o = h2o.H2OFrame(train_df)\n",
    "# val_h2o = h2o.H2OFrame(val_df)\n",
    "# test_h2o = h2o.H2OFrame(test_df)\n",
    "\n",
    "# # Define target and features\n",
    "# target = \"yearly_compensation\"\n",
    "# features = train_h2o.columns\n",
    "# if target in features:\n",
    "#     features.remove(target)\n",
    "\n",
    "# # Step 2: Train Default Deep Learning Model\n",
    "# default_dl_model = h2o.estimators.H2ODeepLearningEstimator(seed=42)\n",
    "# default_dl_model.train(x=features, y=target, training_frame=train_h2o, validation_frame=val_h2o)\n",
    "\n",
    "# # Evaluate Default Model\n",
    "# default_performance = default_dl_model.model_performance(test_h2o)\n",
    "# print(\"Default Deep Learning Model Performance:\")\n",
    "# print(default_performance)\n",
    "\n",
    "# # Step 3: Hyperparameter Optimization using Optuna\n",
    "# def objective(trial):\n",
    "#     # Define hyperparameter search space\n",
    "#     params = {\n",
    "#         \"epochs\": trial.suggest_int(\"epochs\", 10, 100),\n",
    "#         \"hidden\": trial.suggest_categorical(\"hidden\", [[50, 50], [100, 100], [200, 200]]),\n",
    "#         \"input_dropout_ratio\": trial.suggest_uniform(\"input_dropout_ratio\", 0.0, 0.5),\n",
    "#         \"l1\": trial.suggest_loguniform(\"l1\", 1e-6, 1e-3),\n",
    "#         \"l2\": trial.suggest_loguniform(\"l2\", 1e-6, 1e-3),\n",
    "#         \"activation\": trial.suggest_categorical(\"activation\", [\"Rectifier\", \"Tanh\", \"Maxout\"]),\n",
    "#     }\n",
    "    \n",
    "#     # Train Deep Learning Model with hyperparameters\n",
    "#     model = h2o.estimators.H2ODeepLearningEstimator(**params, seed=42)\n",
    "#     model.train(x=features, y=target, training_frame=train_h2o, validation_frame=val_h2o)\n",
    "    \n",
    "#     # Get validation performance (use RMSE as optimization target)\n",
    "#     performance = model.model_performance(val_h2o)\n",
    "#     return performance.rmse()\n",
    "\n",
    "# # Perform hyperparameter optimization\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=20)\n",
    "\n",
    "# # Best Hyperparameters\n",
    "# best_params = study.best_params\n",
    "# print(\"Best Hyperparameters:\")\n",
    "# print(best_params)\n",
    "\n",
    "# # Step 4: Train Deep Learning Model with Best Hyperparameters\n",
    "# tuned_dl_model = h2o.estimators.H2ODeepLearningEstimator(**best_params, seed=42)\n",
    "# tuned_dl_model.train(x=features, y=target, training_frame=train_h2o, validation_frame=val_h2o)\n",
    "\n",
    "# # Evaluate Tuned Model\n",
    "# tuned_performance = tuned_dl_model.model_performance(test_h2o)\n",
    "# print(\"Tuned Deep Learning Model Performance:\")\n",
    "# print(tuned_performance)\n",
    "\n",
    "# # Step 5: Compare Predictions with Actual\n",
    "# predictions = tuned_dl_model.predict(test_h2o)\n",
    "# actual_values = y_test.to_numpy().ravel()\n",
    "# rounded_predictions = predictions.as_data_frame().to_numpy().ravel().round().astype(int)\n",
    "\n",
    "# accuracy = accuracy_score(actual_values, rounded_predictions)\n",
    "# print(f\"Accuracy on Test Data: {accuracy:.2f}\")\n",
    "\n",
    "# # Step 6: Save the Tuned Model\n",
    "# best_model_path = h2o.save_model(tuned_dl_model, path=\"../04_modelling/models/\")\n",
    "# print(f\"Tuned model saved to: {best_model_path}\")\n",
    "\n",
    "# # Shutdown H2O Cluster\n",
    "# h2o.cluster().shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grid Search for Hyperparameter Tuning\n",
    "# hyper_params = {\n",
    "#     \"epochs\": [50, 100, 150],\n",
    "#     \"hidden\": [[50, 50], [100, 100], [200, 200]],\n",
    "#     \"input_dropout_ratio\": [0.0, 0.2, 0.4],\n",
    "#     \"l1\": [1e-5, 1e-4, 1e-3],\n",
    "#     \"l2\": [1e-5, 1e-4, 1e-3],\n",
    "#     \"activation\": [\"Rectifier\", \"Tanh\", \"Maxout\"]\n",
    "# }\n",
    "\n",
    "# grid_search = H2OGridSearch(\n",
    "#     H2ODeepLearningEstimator(seed=42),\n",
    "#     hyper_params=hyper_params\n",
    "# )\n",
    "\n",
    "# # Train models with grid search\n",
    "# grid_search.train(x=features, y=target, training_frame=train_h2o, validation_frame=val_h2o)\n",
    "\n",
    "# # Get the best model from the grid search\n",
    "# best_model = grid_search.get_grid(sort_by=\"rmse\", decreasing=False).models[0]\n",
    "# print(\"Best Grid Search Model:\")\n",
    "# print(best_model)\n",
    "\n",
    "# # Evaluate the best grid search model\n",
    "# performance = best_model.model_performance(test_h2o)\n",
    "# print(\"Best Grid Search Model Performance:\")\n",
    "# print(performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Search for Hyperparameter Tuning\n",
    "# from h2o.grid.grid_search import H2ORandomGridSearch\n",
    "\n",
    "# hyper_params = {\n",
    "#     \"epochs\": list(range(50, 201, 50)),\n",
    "#     \"hidden\": [[50, 50], [100, 100], [200, 200]],\n",
    "#     \"input_dropout_ratio\": [i / 10.0 for i in range(0, 6)],\n",
    "#     \"l1\": [1e-6, 1e-5, 1e-4],\n",
    "#     \"l2\": [1e-6, 1e-5, 1e-4],\n",
    "#     \"activation\": [\"Rectifier\", \"Tanh\", \"Maxout\"]\n",
    "# }\n",
    "\n",
    "# random_search = H2ORandomGridSearch(\n",
    "#     H2ODeepLearningEstimator(seed=42),\n",
    "#     hyper_params=hyper_params,\n",
    "#     max_models=20  # Limit the number of models\n",
    "# )\n",
    "\n",
    "# # Train models with random search\n",
    "# random_search.train(x=features, y=target, training_frame=train_h2o, validation_frame=val_h2o)\n",
    "\n",
    "# # Get the best model from the random search\n",
    "# best_random_model = random_search.get_grid(sort_by=\"rmse\", decreasing=False).models[0]\n",
    "# print(\"Best Random Search Model:\")\n",
    "# print(best_random_model)\n",
    "\n",
    "# # Evaluate the best random search model\n",
    "# performance = best_random_model.model_performance(test_h2o)\n",
    "# print(\"Best Random Search Model Performance:\")\n",
    "# print(performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Early Stopping with Default Parameters\n",
    "# model_with_early_stopping = h2o.estimators.H2ODeepLearningEstimator(\n",
    "#     stopping_metric=\"rmse\",\n",
    "#     stopping_rounds=5,   # Stop if no improvement in 5 rounds\n",
    "#     stopping_tolerance=0.01,\n",
    "#     epochs=200,\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# model_with_early_stopping.train(x=features, y=target, training_frame=train_h2o, validation_frame=val_h2o)\n",
    "\n",
    "# # Evaluate the model\n",
    "# performance = model_with_early_stopping.model_performance(test_h2o)\n",
    "# print(\"Model Performance with Early Stopping:\")\n",
    "# print(performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train Multiple Models for Ensembling\n",
    "# models = []\n",
    "# for hidden in [[50, 50], [100, 100], [200, 200]]:\n",
    "#     model = h2o.estimators.H2ODeepLearningEstimator(\n",
    "#         hidden=hidden,\n",
    "#         epochs=100,\n",
    "#         seed=42\n",
    "#     )\n",
    "#     model.train(x=features, y=target, training_frame=train_h2o, validation_frame=val_h2o)\n",
    "#     models.append(model)\n",
    "\n",
    "# # Ensemble Predictions (Averaging)\n",
    "# predictions = [model.predict(test_h2o).as_data_frame().to_numpy().ravel() for model in models]\n",
    "# ensemble_prediction = sum(predictions) / len(predictions)\n",
    "\n",
    "# # Evaluate Ensemble\n",
    "# from sklearn.metrics import root_mean_squared_error \n",
    "# rmse_ensemble = root_mean_squared_error(y_test.to_numpy().ravel(), ensemble_prediction, squared=False)\n",
    "# print(f\"RMSE for Ensemble: {rmse_ensemble:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
