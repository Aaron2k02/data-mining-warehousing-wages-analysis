{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a threshold for frequency under which categories are grouped as \"Other\"\n",
    "threshold = 0.01  # 1%\n",
    "total_count = len(preprocessed_df)\n",
    "value_counts = preprocessed_df['What programming language would you recommend an aspiring data scientist to learn first?'].value_counts(normalize=True)\n",
    "\n",
    "# Group rare categories below threshold frequency\n",
    "preprocessed_df['What programming language would you recommend an aspiring data scientist to learn first?'] = preprocessed_df['What programming language would you recommend an aspiring data scientist to learn first?'].apply(lambda x: x if value_counts[x] >= threshold else 'Other')\n",
    "\n",
    "# Check the new distribution\n",
    "print(preprocessed_df['What programming language would you recommend an aspiring data scientist to learn first?'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Group Low-Frequency Categories\n",
    "threshold = 0.01  # 1% threshold\n",
    "value_counts = preprocessed_df['What programming language would you recommend an aspiring data scientist to learn first?'].value_counts(normalize=True)\n",
    "preprocessed_df['What programming language would you recommend an aspiring data scientist to learn first?'] = preprocessed_df['What programming language would you recommend an aspiring data scientist to learn first?'].apply(\n",
    "    lambda x: x if value_counts[x] >= threshold else 'Other'\n",
    ")\n",
    "\n",
    "# Step 2: One-Hot Encode the Grouped Data\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_data = encoder.fit_transform(preprocessed_df[['What programming language would you recommend an aspiring data scientist to learn first?']])\n",
    "\n",
    "# Convert encoded data to DataFrame for easier handling (optional)\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# Step 3: Apply PCA for Dimensionality Reduction\n",
    "# Adjust n_components based on the level of reduction desired\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(encoded_df)\n",
    "\n",
    "# Convert PCA results to a DataFrame and concatenate with original DataFrame if needed\n",
    "pca_df = pd.DataFrame(pca_data, columns=['PCA1', 'PCA2'])\n",
    "preprocessed_df = pd.concat([preprocessed_df, pca_df], axis=1)\n",
    "\n",
    "# Check the result\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Check for and handle missing values in the relevant column\n",
    "preprocessed_df['What programming language would you recommend an aspiring data scientist to learn first?'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Step 2: Group Low-Frequency Categories\n",
    "threshold = 0.01  # 1% threshold\n",
    "value_counts = preprocessed_df['What programming language would you recommend an aspiring data scientist to learn first?'].value_counts(normalize=True)\n",
    "preprocessed_df['What programming language would you recommend an aspiring data scientist to learn first?'] = preprocessed_df['What programming language would you recommend an aspiring data scientist to learn first?'].apply(\n",
    "    lambda x: x if value_counts[x] >= threshold else 'Other'\n",
    ")\n",
    "\n",
    "# Step 3: One-Hot Encode the Grouped Data\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_data = encoder.fit_transform(preprocessed_df[['What programming language would you recommend an aspiring data scientist to learn first?']])\n",
    "\n",
    "# Convert encoded data to DataFrame and ensure row alignment\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out())\n",
    "encoded_df.index = preprocessed_df.index  # align indices with the original DataFrame\n",
    "\n",
    "# Step 4: Apply PCA for Dimensionality Reduction\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(encoded_df)\n",
    "\n",
    "# Convert PCA results to DataFrame and align indices\n",
    "pca_df = pd.DataFrame(pca_data, columns=['PCA1', 'PCA2'], index=preprocessed_df.index)\n",
    "\n",
    "# Step 5: Concatenate PCA results with original DataFrame\n",
    "preprocessed_df = pd.concat([preprocessed_df, pca_df], axis=1)\n",
    "\n",
    "# Check the result for any increase in row count or unexpected NaNs\n",
    "preprocessed_df.shape\n",
    "print(preprocessed_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Frequency Encode the Categorical Column\n",
    "# Get the counts for each category\n",
    "value_counts = preprocessed_df['What programming language would you recommend an aspiring data scientist to learn first?'].value_counts()\n",
    "\n",
    "# Map each category to its frequency count\n",
    "preprocessed_df['language_freq'] = preprocessed_df['What programming language would you recommend an aspiring data scientist to learn first?'].map(value_counts)\n",
    "\n",
    "# Step 2: Apply Log Transformation to Reduce Skew\n",
    "# This will reduce the dominance of the majority class by scaling down large counts\n",
    "preprocessed_df['language_freq'] = np.log1p(preprocessed_df['language_freq'])\n",
    "\n",
    "# Step 3: Apply PCA (if needed for dimensionality reduction)\n",
    "# Here we only have one feature (language_freq), so PCA is not necessary. However, if you had multiple columns,\n",
    "# you could combine them and apply PCA here.\n",
    "\n",
    "# Step 4: Drop the original categorical column\n",
    "preprocessed_df = preprocessed_df.drop(columns=['What programming language would you recommend an aspiring data scientist to learn first?'])\n",
    "\n",
    "# Check the new distribution\n",
    "print(preprocessed_df['language_freq'].describe())\n",
    "print(preprocessed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Header contains space and inproper naming convention usage\n",
    "from dataprep.clean import clean_headers\n",
    "\n",
    "# Format the data headers to snakecase \n",
    "# preprocessed_df = clean_headers(preprocessed_df, case=\"snake\") , remove_accents=False)\n",
    "preprocessed_df = clean_headers(preprocessed_df, remove_accents=False) \n",
    "\n",
    "# Check for the formatted columns names \n",
    "print(preprocessed_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Creation\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df[\"Selections\"] = preprocessed_df['Who/what are your favorite media sources that report on data science topics?']\n",
    "\n",
    "# Step 1: Remove text within parentheses and split options\n",
    "df[\"Selections_Clean\"] = df[\"Selections\"].apply(lambda x: re.sub(r'\\s*\\([^)]*\\)\", \"', x).split(\", \"))\n",
    "\n",
    "# Step 2: Get unique options across all rows\n",
    "unique_options = set(option.strip() for row in df[\"Selections_Clean\"] for option in row)\n",
    "\n",
    "# Step 3: Create a column for each unique option and mark as 'yes' or 'no'\n",
    "for option in unique_options:\n",
    "    df[option] = df[\"Selections_Clean\"].apply(lambda x: \"yes\" if option in x else \"no\")\n",
    "\n",
    "# Drop the intermediate cleaned column if not needed\n",
    "df.drop(columns=[\"Selections_Clean\"], inplace=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Simplify the responses by extracting the main category\n",
    "df['Simplified_Sources'] = preprocessed_df['Who/what are your favorite media sources that report on data science topics?'].str.replace(r\"\\s*\\(.*?\\)\", \"\")\n",
    "\n",
    "# Step 2: Split the sources by delimiter to get lists of sources\n",
    "df['Source_List'] = df['Simplified_Sources'].str.split(', ')\n",
    "df['Source_List'] \n",
    "\n",
    "# Convert lists to strings\n",
    "df['Source_List'] = df['Source_List'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Assign to preprocessed_df\n",
    "preprocessed_df['Who/what are your favorite media sources that report on data science topics?'] = df['Simplified_Sources']\n",
    "preprocessed_df['Who/what are your favorite media sources that report on data science topics?'] \n",
    "\n",
    "# Step 3: Get unique sources to create columns\n",
    "unique_sources = set([source for sources in df['Source_List'] for source in sources])\n",
    "unique_sources\n",
    "\n",
    "# Step 4: Create binary columns for each unique source\n",
    "for source in unique_sources:\n",
    "    preprocessed_df[source] = df['Source_List'].apply(lambda x: 1 if source in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the column\n",
    "company_size_dummies = pd.get_dummies(preprocessed_df['What is the size of the company where you are employed?'], prefix=\"Company_Size\")\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "df_test = pd.concat([preprocessed_df, company_size_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to split and clean min and max compensation\n",
    "def split_compensation(df, column):\n",
    "    # Copy the DataFrame to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create two new columns for min and max compensation\n",
    "    df.loc[:, 'Minimum Compensation (USD)'] = df[column].str.extract(r'\\$?(\\d+[,]*\\d*)')[0].str.replace(',', '')\n",
    "    df.loc[:, 'Maximum Compensation (USD)'] = df[column].str.extract(r'-(\\d+[,]*\\d*)')[0].str.replace(',', '')\n",
    "    \n",
    "    # Handle cases with only a minimum (e.g., \"$100,000+\")\n",
    "    df.loc[:, 'Maximum Compensation (USD)'] = np.where(df['Maximum Compensation (USD)'].isna(), np.nan, df['Maximum Compensation (USD)'])\n",
    "    \n",
    "    # Convert the min and max compensation to integers, handling NaN values\n",
    "    df.loc[:, 'Minimum Compensation (USD)'] = df['Minimum Compensation (USD)'].fillna(0).astype(int)\n",
    "    df.loc[:, 'Maximum Compensation (USD)'] = df['Maximum Compensation (USD)'].fillna(0).astype(int)\n",
    "    \n",
    "    # Drop the original column\n",
    "    # df.drop(columns=[column], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "preprocessed_df = split_compensation(preprocessed_df, 'What is your current yearly compensation (approximate $USD)?')\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(preprocessed_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
